import ply.lex as lex
from newParser import parser
from semantics import SemanticAnalyzer
import tokens
import sys

try:
    with open('example.txt', 'r') as file:
        file_contents = file.read()
except FileNotFoundError:
    print("File 'example.txt' not found.")
    sys.exit(1)
except IOError as e:
    print(f"Error reading file: {e}")
    sys.exit(1)

# Build the lexer and pass it the source code
lexer = lex.lex(module=tokens)
lexer.input(file_contents)

# Print tokens generated by the lexer
for token in lexer:
    print(token)

# Reset lexer for parser
lexer.input(file_contents)

# Parse the input
result = parser.parse(lexer=lexer)

print(result)

# Create an instance of the semantic analyzer
semantic_analyzer = SemanticAnalyzer()

# Analyze the generated AST
try:
    semantic_analyzer.analyze(result)
    print("Semantic analysis successful.")
except Exception as e:
    print(f"Semantic analysis failed: {e}")
